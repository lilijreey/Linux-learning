
num  0cc55700

Big endian 低有效字节在最大地址开始向小地址
Address: 00	01	03	04
data:    00	0c     c5	57

little endian 低有效字节在最小地址开始向大地址
Address: 00	01	03	04
data:    57	c5	0c	00


TCP {{{
===================================
Qus. TCP 何时通知应用层有数据到了??
TCP 是一种流协议：
   没有固定报文边界的协议，读取TCP数据就向从串口读数据一样，
   无法预先得知在一次读取时会返回多少字节
   send 默认情况下应用层调用send, send在把数据复制到内核TCP栈后就返回了，至于
   何时发送，是不是会和其他send的数据合并成一个pkg发送，都是有内核决定的
   也就是说，tcp实际发送的包和调用send的时候的数据基本上没有直接关系.
   而且读取时也无法判断数据是一次到来的，还是多次到来的.
   所以，不要对read时的数据有任何的假设, 对应用层来说TCP没有分组的概念
   TCP 是面向流动这和UDP不同
   对于设置了PUH 的pkg TCP要求协议站要马上向应用曾推送,不管等等窗口是否
    被填满

##TCP 的几个标志

SYN 建立链接客户端发送的可一个pkg，就设置SYN
ACK
RST
    产生RST的三种情况
   1. 一个没有带开的端口受到一个connect请求, 访问一个不存在的端口
   2. 异常终止一个链接， 使用 socket SO_LINGER 来发送RST报文
   2. 对一个已经关闭的端口写数据

FIN 关闭连接时设置
PSH PUSH
URG

##TCP 链接的11个状态

FIN_WAIT_2 主动close 方， 在受到自己发送的FIN 的ACK后，为FIN_WAIT_2
                          这时等待 server 发送的FIN
                          如果serve 永远不发送FIN，或者没有等到server
               的FIN就强制退出了，则这时这个链接有kernel接管，
                叫做孤儿链接(不属于任何进程的链接)， 
               Linux为了防止孤儿链接长时间存留在内核
                指定了两个变量 /proc/sys/net/ipv4/tcp_max_orphans
                   内核最多的孤儿链接数量,如果超过这个值，新的孤儿链接
                   会被直接reset
                 /proc/sys/net/ipv4/tcp_fin_timeout
                   孤儿链接在内核中生存的时间,也就是一个孤儿链接
                   FIN_WAIT_2 状态应该保持多长时间

Qus.
1. TCP 不断接收包，但是一直不读取，
      这些包满了后会被丢弃吗？
      write的时候会因为read buf已满出错吗？
   不会如果应用曾一直不读取，会导致发送方不发送新的数据， （流量控制)

TCP 连接关闭情况

1.正常关闭 A: close. 给B发送 FIN包
  B: 调用read 返回0. 表示A：已经关闭（不能在发送数据了)但是还可以接受数据
  Qus. 如果B不掉用 read可以知道A：发送的FIN吗？

2.server段程序崩溃。system 会发送FIN 包
   client 段也许会知道(操作系统为打开的fd调用了close）
                      也可能不会知道,两中情况
    另一端会立即知道
    TEST
    B： 调用read的会收到 EOF,或一直阻塞
    C:  调用write ok, 在read时会收到RST, 收到RST后内核会断开TCP链接
        如果忽略RST 错误，再次调用write 会收到SIGPIPE.

3. server 主机崩溃 不会发送FIN 包
    另一不会收到任何信息，在主机重启之前这种情况看起了和网络故障
    一样, write 发送的msg都会tiemout 返回ETIMEDOUT
   

4. server主机崩溃并重启
     server会收到一个client的重传data，而
     server不知如何处理他，server会给client发送一条RST，并断开链接
     client 收到一个RST后会丢弃链接
 A:调用read时server以重启: 
       client read返回EOF或其他错误
 B:调用write时server以重启: SIGPIPE

3. A突然断电，网线断掉：
   所有操作都阻塞，或超时


Socket API 注意是不是可重入的

|socket|/3
  不是每个参数都可以相互匹配
  3个参数有有效的匹配
  

|connect| 调用发起3次握手，知道连接建立或者失败才返回
收到第二分解 peer SYN(ACK)包. 返回
如果connect 失败则socket 不可再用。必须关闭. TCP
因为connect failed 后 socket的状态为定义
* 异步connect 过程
   EINPROGRESS
    

返回失败的几种情况:
1. 对方没有开机: 对方太忙，
  始终没有收到对方的ACK，则返回ETIMEDOUT

2. Peer 对应服务没有启动。 连接端口没有监听进程
  Peer 回复 RST connect 返回 ECONNREFUSED

3. 路由不可达

4. 对方主机可链接数量满了,返回ECONNREFUSED

|bind| 绑定socket到一个协议地址

如果不帮定。那么在connect 或 listen 时，内核就要为socket
选择一个临时*port* *IP* 对于client bind无所谓。但是对用
server 通常都需要一个固定的port

IPaddrs: 进程绑定的ip地址必须是进程所在主机的一个接口
绑定IP后, 进程所有发送的包的源地址IP就是绑定的IP.
进程只能接受目的地址是绑定IP的包

如果TCP server不绑定IP，则Server的IP会是 client SYN包的目的IP

IP: 统配地址 ipv4 INADDR_ANY, 内核指定, 何时指定？
             ipv6 in6addr_any. 
通配符不同的原因:
    INADDR_ANY 通常是0.可以使用一个数字表示
    ipV6 地址是个结构，不能使用0来表示 port： 0, 内核指定 目的IP 和 到达接口
    INADDR_LOOPBACK

INADDR_ANY是ANY，是绑定地址0.0.0.0上的监听, 能收到任意一块网卡的连接；
INADDR_LOOPBACK, 也就是绑定地址LOOPBAC, 往往是127.0.0.1, 只能收到127.0.0.1上面的连接请求 

INADDR_NONE 是32位均为1的值（即255.255.255.255，它是Internet的有限广播地址）
如何一个主机有多个IP地址。情况是什么?


得到connect后内核分配的地址
getsockname  //通常client调用，得到自己的
getpeerkname //通常server调用，得到client的

|listen/2|
做两件事
1.把fd从主动接口，设置为被动接口.监听状态 
  当主机收到一个链接请求时，kernel就会查找监听socket列表
  socket/3默认返回一个主动接口
  指示内核因该接受指向此接口的连接请求.
  TCP 状态从 CLOSED 到 LISTEN
2.设置请求队列大小
  The  behavior of the backlog argument on TCP sockets changed with Linux
  2.2.  Now it specifies the  queue  length  for  completely  established
  sockets	waiting  to  be  accepted, instead of the number of incomplete
  connection requests.  The maximum length of the	queue  for  incomplete
  sockets	can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When
  syncookies are enabled there is no logical maximum length and this set‐
  ting is ignored.  See tcp(7) for more information.
  more see *listen(3)*

3. backlog 参数的含义:
       从linux2.2 开始， backlog定义为:
        已经完成链接但还未accepted的 sockets 等待队列大小
        backlog 最大不会大于 /proc/sys/net/core/somaxconn 
        需要修改somaxconn 

4. 未完成的链接队列的大小由 /proc/sys/net/ipv4/tcp_max_syn_backlog 指定


|accept|
Qus. 返回的socket的源地址和目的地址于listen的有何不同
     2. accept 返回一个新连接时完成3此握手了吗
     不一定， accept只是从监听队列中取走链接，而不关心链接处于何种
      状态。也就是说， 任何状态的socket都有可能被accept返回
          比如 CLOSE_WAIT
     epoll listen fd 返回 EPOIN 时 TCP3此握手完成了吗
返回-1 时的errno
  EINTR
  accept被打断，可以再次调用accept
  EAGAIN 非阻塞listen fd 没有等待的连接

  Client                         Server
----------------------------------------------------

connect /1   --> --> SYN C --> -->      Accept/1 blocking
                                       /
   |    <--  SYN K, ACK C+1      <---
connect return  \  -----> ACK K+1 -->   -->  | accept returen



EPIPE:
写一个已经收到FIN包的接口是ok的。
但是写一个已经收到RST的接口就引发SIGPIPE

ip<->name
|gethostbyname|
|gethostbyaddr|
不可重入

使用这些函数主机要联网
使用2_r version

通过 /etc/resolv.conf 文件来查询
如果查询不到，则通过UDP相服务器查询
不通过DNS，也可在 /etc/hosts(静态主机文件)中查找

|gethostname|
return same as hostname(1)
不可重入

|getaddrinfo|
比gethostbyname 更高级的函数
屏蔽了相关协议，
|freeaddrinfo|
不可重入

port<->service-name
|getservbyname|
|gettservbport|
通过 /etc/service 文件查询


|getifaddrs|
 返回本机所有的网络接口

|getprotobynumber|

EE 健壮SOCKET 程序需要注意的几点
  0. 检查系统调用返回值
  1. 捕捉信号：
     +. SIGPIPE. 
        给一个对方已经关闭的socket写数据时产生
          通常做忽略处理.
     +. SIGURG
        紧急IO处理，外带数据OOB到来时产生, I/O 福用时产生
             使用 sockatmark/1(fd) 来判断是否有OOB，如果返回1,则有，
             在使用带 MSG_OOB 的 recv 来接受外带数据
          可以log出来
     +. SIGCHLD
        多进程模式下，通常检测SIGCHLD来wait结束的进程
             while(waitpid(-1,0,WNOHANG) > 0);
     +. SIGHUP
         在守护进程模式下，通常使用SIGHUP信号来让程序restart
     +. SIGIO
         ???
     +. SIGALRM
          需要时处理
   
    2. 资源管理
        CPU memory Processes
           


|setsockopt| more see *tcp(7)*
 level 设置影响的范围
  1. SOL_SOCKET socket 级别

几种常见的选项
1. SO_KEEPALIVE 保活心跳
   用来防止链接的另一端异常终止（即没有发生fin包）导致socket链接一直存在而占用资源

 Qus. 1 keepalive 是自动开启还是需要手动开启
          Ans. 需要手动开启, TCP 默认不开启 keepalive
      2. 每个socket的 keepalive 都可以单独设置,还是统一的
         Ans.   都是单独的 从setsockeopt 可参数就可以看出，
             第一个参数就是需要设置的socketfd
      3. 只有一端开启keepalive 会如何
         Ans. 哪一端想开启keepalive 开启就可以使用， 如果另一端没有开启也不会有问题，
                 仍然会对keepalive 包进行相应。 这是TCP规定的和有没有开启keepalive  无关
      4. sysctl 中的keepalive 选项影响那些
          如果socket开启 keepalive 时的默认值

   默认的设置参数太慢了，所以要自己设定
  * 程序中开启KeepAlive 功能 使用 setsockeopt
      int enable = 1;
    setsockopt(sockfd, SOL_SOCKET, SO_KEEPALIVE, &enable, sizeof(enable))

  * 设置 始首次KeepAlive探测前的TCP空闭时间
      int keepidle =5;
    setsockopt(sockfd, SOL_TCP, TCP_KEEPIDLE, &keepidle, sizeof(keepidle))

  * 两次KeepAlive探测间的时间间隔
    int keepInterval = 5;
    setsockopt(sockfd, SOL_TCP, TCP_KEEPINTVL, &keepInterval, sizeof(keepInterval))

  * 判定断开前的KeepAlive探测次数 
    int keepCount = 3;
    setsockopt(sockfd, SOL_TCP, TCP_KEEPCNT, &keepCount, sizeof(keepCount))

 sysctl 中的设定
        如果socket开启 keepalive 时的默认值
	tcp_keepalive_time = 7200 seconds (2 hours)
	tcp_keepalive_probes = 9
	tcp_keepalive_intvl = 75 seconds
	意思是如果某个TCP连接在idle 2个小时后,内核才发起probe.如果probe 9次(每次75秒)不成功,
                      内核才彻底放弃,认为该连接已失效.对服务器而言,显然上述值太大. 可调整到:
	/proc/sys/net/ipv4/tcp_keepalive_time 1800
	/proc/sys/net/ipv4/tcp_keepalive_intvl 30
	/proc/sys/net/ipv4/tcp_keepalive_probes 3
2. SO_RCVBUF, SO_SNDBUF
  接收发送缓存大小，tcp窗口的小
  必须在connect bind前调用
  linux  SO_RCVBUF=87380, SO_SNDBUF=16384
  
  每个TCP链接都维护一个接收窗口和发送窗口
  窗口主要用于流量控制，和超时重传

  receive winodw
  +----+----+----+----+----+
  | 14 | 15 | 16 | 17 | 18 |
  +----+----+----+----+----+
  左边是期望收到的下一个segment的序号，右边是可以接受的最大的segment的序号，
  tcp 只会接收在窗口中的segment，也就是说 不再receive-winodw 序号中的segment都将被抛弃


  send window
  +----+----+----+----+----+
  | 14 | 15 | 16 | 17 | 18 |
  +----+----+----+----+----+
                 ^
                 |
       next-send/
  send-window 有已发送还被确认的segment和可以发送还没发送的segment组成

窗口扩大因子
  TCP 头部的接受窗口大小为16bit，最多为65535字节
  但是TCP实际上允许的窗口大小比这个打的多，为了提高TCP的吞吐量
  窗口扩大因子，解决了这个问题。
   /proc/sys/net/ipv4/tcp_window_scaling
    有效值0～14, 意思是 比如TCP的实际接受窗口大小为N，则把N左移 M位
    窗口扩大因子的设置实在链接建立时 SYN
     pkg来制定的，整个链接中扩大因子不会在改变了



3. SO_RCVTIMEO SO_SNDTIMEO
  接收发送超时设置. default is 0

  SO_RCVLOWAT SO_SENDLOWAT 
   表示tcp接收和发送缓冲的低水位标志，他们一般别IO服用系统调用，
    来判断socket是否可读写, 当缓存中实际的data 大于 SO_RCVLOWAT 时
    IO服用系统将通知应用层该socket可读，
     当发送缓冲的data 大于 SO_SENDLOWAT 时， IO服用系统将通知 应用层
     该socket 可写

4. SO_REUSEADDR SO_REUSEPORT
   bind 之前设置
  允许重用地址， 在time_wait 状态下的地址是否可以重用
   一般用于重启后马上要重新绑定到原来地址的
   也可一通过 /proc/sys/net/ipv4/tcp_tw_recycle
    来开启快速回收被关闭的socket， 需要和 /proc/sys/net/ipv4/tcp_timestamps
       同时打开 才有效
     2. /proc/sys/net/ipv4/tcp_tw_reuse 允许将 TIME_WAIT socket重用于新的socket
        也必须和/proc/sys/net/ipv4/tcp_timestamps 同时打开才有效


5. TCP_NODELAY
   禁止Nagle算法
   不会把小包拼接成大包后在发送，会使得小包用户的体验增强, 但是这会
   降低对TCP的利用率， 因为有效载荷很少，但是每个pkg的固定长度都是一样的

  Nagle 算法：
     1. 把受到的第一个数据块发出去，
     2. 直到收到所有上次发出去的pkg的ack，或者已经累计的足够的数据包
     3. 重复2

   TCP_CORK
   和TCP_NODELAY 相反，他是告诉kernel即使app write数据到kernel，也不要发送
   一般是在sendfile时使用，因为每个file都需要前写入一些头,在写入后要sendfile
   使用TCP_CORK 就可以让kernel都到sendfile完成后，一起发送（取消TCP_CORK)
   在BSD上面TCP_CORK 有形象的叫做塞子，和拔塞子 NOPUSH, PUSH

#define ngx_tcp_nopush_n   "setsockopt(TCP_CORK)"
#define ngx_tcp_push_n     "setsockopt(!TCP_CORK)"
   

int
ngx_tcp_nopush(ngx_socket_t s)
{
    int  tcp_nopush;

    tcp_nopush = 1;

    return setsockopt(s, IPPROTO_TCP, TCP_NOPUSH,
                      (const void *) &tcp_nopush, sizeof(int));
}


为什么TCP_QUICKACK需要在每次recv后重新设置？
      因为TCP_QUICKACK不是永久的，所以在每次recv数据后，应该重新设置。

 推迟确认， 机制。 在使用 Nagle 算法是用来避免 糊涂窗口问题
   /proc/sys/net/ipv4/tcp_delack_min

6. TCP_DEFER_ACCEPT
   当连接请求由数据到达是在返回。 用于拒绝接收只建立连接不发送数据的请求。
   
7. SO_LINGER
   用于区分，正常关闭和异常关闭
   改变 close 的默认动作，通常close立即返回。如果close时还有没有发出的msg
   系统将试着把这些msg发出
   传递参数 linger { int l_onoff /*0 is off*/
                     int l_linger 
                    }
  l_onoff = 0 等于没有设置,close 默认动作
  l_onoff != 0 ??? linux 

|write|| |send/4| 
send 和write的前3个参数相同，但是有一个flags 参数
比write 提供更多的控制
 send 只有在socket是链接状态下,才能调用
 write(socket, buf, len) == send(socket, buf, len, 0)
 flags:
     MSG_OOB 外带数据
     MSG_NOSIGNAL 方要发生SIGPIPE 时不发生
          当时会返回-1 erron 会设为 EPIPE


|sendto|/6
  一般用于UDP ,TCP也可以用, 

  send(socket,buf,len,flags) = sendto(socket,buf,len,flags, NULL, 0)
  if sendto用于面向链接的 socket, (SOCK_STREAM, SOCK_SEQPACKET)
   后两个参数将被忽略, 这时后面两个参数必须为NULL，0，否则会返回
   错误 EISCONN

|recvfrom|
  用于UDP 接受

|inet_addr|
   x.x.x. 转化为二进制地址
  e.g. sockaddr_in addr.sin_addr.s_addr = inet_addr(x.x.x.)
 e.g.

brocdcase
 +. 有限广播地址
   255.255.255.255 路由不会转发此地址的msg，也就是说这个msg不会跑的子网
   以外的地方。 主要用来得到自己的IP
 +. 网络广播
   向一个指定网络的所有主机广播msg
   保留网络地址，主机地址全部为255
     e.g. 190.25.255.255 广播个190.25 网络中的所有主机
    路由不一定支持^_^
 +.
     
EE SYN Flood 攻击
 原理。。。。
 预防
 |syncookies| SYN Cookie
  一种预防SYN Flood 的方法 see .syncookie.readme
   

EE 网络内核参数 files
  /proc/sys/net/core/somaxconn 已经完成链接还没有accept的socket队列大小
        listen()的默认参数,挂起请求的最大数量.默认是128.对繁忙的服务器,增加该值有助于网络性能.
        可通过/etc/sysctl.conf 中添加net.core.somaxconn = 2048 来修改
        在执行 sysctl -p
  /etc/sysctl.conf
	# /etc/sysctl.conf - Configuration file for setting system variables
	# See /etc/sysctl.d/ for additional system variables
	# See sysctl.conf (5) for information.

 /proc/sys/net/core/wmem_max
   最大socket写buffer,可参考的优化值:873200
 /proc/sys/net/core/rmem_max
   最大socket读buffer,可参考的优化值:873200


	/proc/sys/net/core/rmem_default 
            定义默认的接收窗口大小；对于更大的 BDP 来说，这个大小也应该更大。
	/proc/sys/net/core/rmem_max 
 	 	定义接收窗口的最大大小；对于更大的 BDP 来说，这个大小也应该更大。
	/proc/sys/net/core/wmem_default 	
            定义默认的发送窗口大小；对于更大的 BDP 来说，这个大小也应该更大。
	/proc/sys/net/core/wmem_max 	
          	定义发送窗口的最大大小；对于更大的 BDP 来说，这个大小也应该更大。
	/proc/sys/net/ipv4/tcp_window_scaling 	
             	启用 RFC 1323 定义的 window scaling；扩大因子 要支持超过 64KB 的窗口，必须启用该值。
	/proc/sys/net/ipv4/tcp_sack  
	启用有选择的应答（Selective Acknowledgment），这可以通过有选择地应答乱序接收到的报文来提高性能
            （这样可以让发送者只发送丢失的报文段）；（对于广域网通信来说）这个选项应该启用，但是这会增加对 CPU 的占用。
	/proc/sys/net/ipv4/tcp_fack 	
                启用转发应答（Forward Acknowledgment），这可以进行有选择应答（SACK）从而减少拥塞情况的发生；这个选项也应该启用。
	/proc/sys/net/ipv4/tcp_timestamps 	以一种比重发超时更精确的方法
                    （请参阅 RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项。
	/proc/sys/net/ipv4/tcp_wmem 	
                    为自动调优定义每个 socket 使用的内存。第一个值是为 socket 的发送缓冲区分配的最少字节数。
                   第二个值是默认值（该值会被 wmem_default 覆盖），缓冲区在系统负载不重的情况下可以增长到这个值
                  。第三个值是发送缓冲区空间的最大字节数（该值会被 wmem_max 覆盖）。
	/proc/sys/net/ipv4/tcp_rmem 	
              	与 tcp_wmem 类似，不过它表示的是为自动调优所使用的接收缓冲区的值。
	/proc/sys/net/ipv4/tcp_low_latency 	
             允许 TCP/IP 栈适应在高吞吐量情况下低延时的情况；这个选项应该禁用。
	/proc/sys/net/ipv4/tcp_westwood 	
                启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化；对于 WAN 通信来说应该启用这个选项。
	/proc/sys/net/ipv4/tcp_bic 
         	为快速长距离网络启用 Binary Increase Congestion；这样可以更好地利用以 GB 速度进行操作的链接；对于 WAN 通信应该启用这个选项。

 /proc/sys/net/ipv4/tcp_mem
	同样有3个值,意思是:
	net.ipv4.tcp_mem[0]:低于此值,TCP没有内存压力.
	net.ipv4.tcp_mem[1]:在此值下,进入内存压力阶段.
	net.ipv4.tcp_mem[2]:高于此值,TCP拒绝分配socket.
	上述内存单位是页,而不是字节.可参考的优化值是:786432 1048576 1572864
         确定 TCP 栈应该如何反映内存使用；每个值的单位都是内存页（通常是 4KB）。
           	第一个值是内存使用的下限。第二个值是内存压力模式开始对缓冲区使用应用压力的上限。
                第三个值是内存上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。
                对于较大的 BDP 可以增大这些值（但是要记住，其单位是内存页，而不是字节）。

 /proc/sys/net/core/netdev_max_backlog
   进入包的最大设备队列.默认是300,对重负载服务器而言,该值太低,可调整到1000.
 /proc/sys/net/core/optmem_max
  socket buffer的最大初始化值,

 /proc/sys/net/ipv4/tcp_max_syn_backlog
   进入SYN包的最大请求队列.默认1024.对重负载服务器,增加该值显然有好处.可调整到2048.
 /proc/sys/net/ipv4/tcp_retries2
   TCP失败重传次数,默认值15,意味着重传15次才彻底放弃.可减少到5,以尽早释放内核资源.


  /proc/sys/net/ipv4/ip_local_port_range
  指定端口范围的一个配置,默认是32768 61000,已够大.

  net.ipv4.tcp_syncookies = 1
     表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
  net.ipv4.tcp_tw_reuse = 1
     表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
  net.ipv4.tcp_tw_recycle = 1
     表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
  net.ipv4.tcp_fin_timeout = 30
     表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。
  net.ipv4.tcp_keepalive_time = 1200
     表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。
  net.ipv4.ip_local_port_range = 1024 65000
     表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。
  net.ipv4.tcp_max_syn_backlog = 8192
     表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。
  net.ipv4.tcp_max_tw_buckets = 5000
     表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为 5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。
 }}} 

UDP
=================================
* UDP 是保存边界的
   就是如果Client 发送一个500B 的数据给Server， 如果这个包
   安全完好的传输到了Server，则server read的时候也会把这个把一次完整
   都取。 也就是说每次write UPD 都会生产一个UDP包, 不同的UDP包
   不会被OS合成一个包， 如果没有包的丢失和损害，则发送端和接受端的包
   对一一对应,这和TCP不同，

* send UDP 时只要 UDP数据被正确复制到OS就返回正确，
* UDP 分包
*     理论上UDP单个包最多是66536B 但是这个会受到MTU的限制，
*     如果UDP包大于MTU则会被分解为多个包，传输，如果传输过程
*     中有任意一个包丢失，则整个数据包就会被抛弃,如果所有分组
*     都被成功传输到接收端则OS会负责重组数据包

proc man upd
* /proc/sys/net/ipv4/udp_mem
   和tcp_mem 一样,配置系统为所有udp socket分配的内存
* /proc/sys/net/ipv4/udp_rmem_min
    系统给每个udp socket 至少分配的read buffer, 即时这时所有的udp使用的内存
        已经超出 udp_mem中的最大值
* /proc/sys/net/ipv4/udp_wmem_min
    系统给每个udp socket 至少分配的write buffer, 即时这时所有的udp使用的内存
        已经超出 udp_mem中的最大值
  

Funcs
=====================================
* 大小端转换
      hton{s,l}
      ntoh{s,l}
  BSD 提供64为的转换
      htole64
      htobe64

* IP
  不管ipV4， V6 都使用struct sockaddr  做表示
    但是在处理中 V4 使用 struct sockaddr_in
     v6 使用 struct sockaddr_in6
   强转就像



* IP 地址转换
    inet_aton()   convert dot-ip to network-ordered binary
    inet_ntoa() 
        已经不建议使用了，使用 inet_pton/ inet_ntop

   DNS gethostname/ human to IP address
   Server Port /etc/services
     getservbyname


vim: ft=help foldmethod=marker
